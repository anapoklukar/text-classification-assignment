{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d317450-1724-4c19-a252-c64ee1291649",
   "metadata": {},
   "source": [
    "# Assignment 2: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af0130-4c6f-47f8-8c35-dd9daa6d007f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4fe8d0-0339-4622-9a46-59f3999afaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# Initialize an empty list to store processed entries\n",
    "processed_data = []\n",
    "\n",
    "# Read the JSON file and process each line\n",
    "with open('News_Category_Dataset_IS_course.json', 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse the JSON data for each line\n",
    "        entry = json.loads(line)\n",
    "\n",
    "        # Extract relevant information\n",
    "        link = entry[\"link\"]\n",
    "        headline = entry[\"headline\"]\n",
    "        category = entry[\"category\"]\n",
    "        short_description = entry[\"short_description\"]\n",
    "        authors = entry[\"authors\"]\n",
    "        \n",
    "        # Convert the date from milliseconds to a human-readable format\n",
    "        date = entry[\"date\"]\n",
    "        formatted_date = datetime.datetime.utcfromtimestamp(date / 1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Store the processed entry in the list\n",
    "        processed_entry = {\n",
    "            \"link\": link,\n",
    "            \"headline\": headline,\n",
    "            \"category\": category,\n",
    "            \"short_description\": short_description,\n",
    "            \"authors\": authors,\n",
    "            \"date\": formatted_date\n",
    "        }\n",
    "        processed_data.append(processed_entry)\n",
    "\n",
    "# Write processed data to a CSV file\n",
    "csv_file_path = 'processed_data.csv'\n",
    "fieldnames = [\"link\", \"headline\", \"category\", \"short_description\", \"authors\", \"date\"]\n",
    "\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write data\n",
    "    for entry in processed_data:\n",
    "        writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b77b21-762b-4c92-8f31-ae1cf38e3a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/dodgers-basebal...</td>\n",
       "      <td>Maury Wills, Base-Stealing Shortstop For Dodge...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Maury Wills, who helped the Los Angeles Dodger...</td>\n",
       "      <td>Beth Harris, AP</td>\n",
       "      <td>2022-09-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/golden-globes-r...</td>\n",
       "      <td>Golden Globes Returning To NBC In January Afte...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>For the past 18 months, Hollywood has effectiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-09-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/biden-us-forces...</td>\n",
       "      <td>Biden Says U.S. Forces Would Defend Taiwan If ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>President issues vow as tensions with China rise.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-09-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148117</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/girl-with...</td>\n",
       "      <td>'Girl With the Dragon Tattoo' India Release Ca...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>\"Sony Pictures will not be releasing The Girl ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148118</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148119</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148120</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148121</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148122 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "0       https://www.huffpost.com/entry/funniest-tweets...   \n",
       "1       https://www.huffpost.com/entry/funniest-parent...   \n",
       "2       https://www.huffpost.com/entry/dodgers-basebal...   \n",
       "3       https://www.huffpost.com/entry/golden-globes-r...   \n",
       "4       https://www.huffpost.com/entry/biden-us-forces...   \n",
       "...                                                   ...   \n",
       "148117  https://www.huffingtonpost.com/entry/girl-with...   \n",
       "148118  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "148119  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "148120  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "148121  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline       category  \\\n",
       "0       23 Of The Funniest Tweets About Cats And Dogs ...         COMEDY   \n",
       "1       The Funniest Tweets From Parents This Week (Se...      PARENTING   \n",
       "2       Maury Wills, Base-Stealing Shortstop For Dodge...         SPORTS   \n",
       "3       Golden Globes Returning To NBC In January Afte...  ENTERTAINMENT   \n",
       "4       Biden Says U.S. Forces Would Defend Taiwan If ...       POLITICS   \n",
       "...                                                   ...            ...   \n",
       "148117  'Girl With the Dragon Tattoo' India Release Ca...  ENTERTAINMENT   \n",
       "148118  Maria Sharapova Stunned By Victoria Azarenka I...         SPORTS   \n",
       "148119  Giants Over Patriots, Jets Over Colts Among  M...         SPORTS   \n",
       "148120  Aldon Smith Arrested: 49ers Linebacker Busted ...         SPORTS   \n",
       "148121  Dwight Howard Rips Teammates After Magic Loss ...         SPORTS   \n",
       "\n",
       "                                        short_description           authors  \\\n",
       "0       \"Until you have a dog you don't understand wha...     Elyse Wanshel   \n",
       "1       \"Accidentally put grown-up toothpaste on my to...  Caroline Bologna   \n",
       "2       Maury Wills, who helped the Los Angeles Dodger...   Beth Harris, AP   \n",
       "3       For the past 18 months, Hollywood has effectiv...               NaN   \n",
       "4       President issues vow as tensions with China rise.               NaN   \n",
       "...                                                   ...               ...   \n",
       "148117  \"Sony Pictures will not be releasing The Girl ...               NaN   \n",
       "148118  Afterward, Azarenka, more effusive with the pr...               NaN   \n",
       "148119  Leading up to Super Bowl XLVI, the most talked...               NaN   \n",
       "148120  CORRECTION: An earlier version of this story i...               NaN   \n",
       "148121  The five-time all-star center tore into his te...               NaN   \n",
       "\n",
       "                       date  \n",
       "0       2022-09-23 00:00:00  \n",
       "1       2022-09-23 00:00:00  \n",
       "2       2022-09-20 00:00:00  \n",
       "3       2022-09-20 00:00:00  \n",
       "4       2022-09-19 00:00:00  \n",
       "...                     ...  \n",
       "148117  2012-01-28 00:00:00  \n",
       "148118  2012-01-28 00:00:00  \n",
       "148119  2012-01-28 00:00:00  \n",
       "148120  2012-01-28 00:00:00  \n",
       "148121  2012-01-28 00:00:00  \n",
       "\n",
       "[148122 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ad0e2b-6138-4d8f-a8ab-adb65a14c0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>148122</td>\n",
       "      <td>147388</td>\n",
       "      <td>148122</td>\n",
       "      <td>135938</td>\n",
       "      <td>123706</td>\n",
       "      <td>148122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>148096</td>\n",
       "      <td>146295</td>\n",
       "      <td>15</td>\n",
       "      <td>133792</td>\n",
       "      <td>19633</td>\n",
       "      <td>3618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://www.huffingtonpost.comhttp://www.newre...</td>\n",
       "      <td>Sunday Roundup</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Welcome to the HuffPost Rise Morning Newsbrief...</td>\n",
       "      <td>Lee Moran</td>\n",
       "      <td>2014-11-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>35602</td>\n",
       "      <td>191</td>\n",
       "      <td>2058</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link        headline  \\\n",
       "count                                              148122          147388   \n",
       "unique                                             148096          146295   \n",
       "top     https://www.huffingtonpost.comhttp://www.newre...  Sunday Roundup   \n",
       "freq                                                    2              90   \n",
       "\n",
       "        category                                  short_description  \\\n",
       "count     148122                                             135938   \n",
       "unique        15                                             133792   \n",
       "top     POLITICS  Welcome to the HuffPost Rise Morning Newsbrief...   \n",
       "freq       35602                                                191   \n",
       "\n",
       "          authors                 date  \n",
       "count      123706               148122  \n",
       "unique      19633                 3618  \n",
       "top     Lee Moran  2014-11-05 00:00:00  \n",
       "freq         2058                   98  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8da2575-182b-4e4f-b088-3a380863a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data Summary:\n",
      "link                     0\n",
      "headline               734\n",
      "category                 0\n",
      "short_description    12184\n",
      "authors              24416\n",
      "date                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data in each column\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf11eb8-8e5e-47a7-b060-4da9df3a6f16",
   "metadata": {},
   "source": [
    "We are not removing the data that has missing short_description and author, since they are a big fraction of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9105b4-4441-4298-8987-b04376813297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link                 object\n",
       "headline             object\n",
       "category             object\n",
       "short_description    object\n",
       "authors              object\n",
       "date                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dta types of columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61674786-a50a-4dfb-a7d4-4f801b3e35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything but date to string\n",
    "df['link'] = df['link'].astype(\"string\")\n",
    "df['headline'] = df['headline'].astype(\"string\")\n",
    "df['category'] = df['category'].astype(\"string\")\n",
    "df['short_description'] = df['short_description'].astype(\"string\")\n",
    "df['authors'] = df['authors'].astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f4e4d-4a0d-4743-bf82-4ffd26e2a2e3",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66c2c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f2b561a-d9e5-4c07-a067-d0132d0d3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary resources (if not already downloaded)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Initialize Lemmatizer and Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9d5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())  # Convert text to lowercase\n",
    "\n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Stemming (uncomment if you want to use stemming)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    preprocessed_text = ' '.join(lemmatized_words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efdb7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess links (remove everything up to /entry/ and remove the '-' characters)\n",
    "def preprocess_link(link):\n",
    "    # Remove everything up to /entry/ if it exists\n",
    "    if '/entry/' in link:\n",
    "        link = link.split('/entry/')[1]\n",
    "    \n",
    "    #Remove everything after the first '_' character if it exists\n",
    "    if '_' in link:\n",
    "        link = link.split('_')[0]\n",
    "\n",
    "    # Remove the '-' characters if they exist\n",
    "    if '-' in link:\n",
    "        link = link.replace('-', ' ')\n",
    "\n",
    "    #preprocess the link text\n",
    "    link = preprocess_text(link)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb5696ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the preprocessing function to the headline column only in rows where the headline isn't missing\n",
    "df.loc[df['headline'].notnull(), 'headline'] = df.loc[df['headline'].notnull(), 'headline'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4be6113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the preprocessing to short_description and links as well\n",
    "df.loc[df['short_description'].notnull(), 'short_description'] = df.loc[df['short_description'].notnull(), 'short_description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2738a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the authors where the authors aren't missing\n",
    "df.loc[df['authors'].notnull(), 'authors'] = df.loc[df['authors'].notnull(), 'authors'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "285ca5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the links where the links aren't missing\n",
    "df.loc[df['link'].notnull(), 'link'] = df.loc[df['link'].notnull(), 'link'].apply(preprocess_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5669dead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "      <td>funniest tweet cat dog september</td>\n",
       "      <td>funniest tweet cat dog week</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>dog understand could eaten</td>\n",
       "      <td>elyse wanshel</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "      <td>funniest parenting tweet</td>\n",
       "      <td>funniest tweet parent week</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>accidentally put toothpaste toddler toothbrush...</td>\n",
       "      <td>caroline bologna</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/dodgers-basebal...</td>\n",
       "      <td>Maury Wills, Base-Stealing Shortstop For Dodge...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Maury Wills, who helped the Los Angeles Dodger...</td>\n",
       "      <td>Beth Harris, AP</td>\n",
       "      <td>2022-09-20 00:00:00</td>\n",
       "      <td>dodger baseball obit will</td>\n",
       "      <td>maury will shortstop dodger dy</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>maury helped los angeles dodger win three worl...</td>\n",
       "      <td>beth harris ap</td>\n",
       "      <td>2022-09-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/golden-globes-r...</td>\n",
       "      <td>Golden Globes Returning To NBC In January Afte...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>For the past 18 months, Hollywood has effectiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-09-20 00:00:00</td>\n",
       "      <td>golden globe return nbc</td>\n",
       "      <td>golden globe returning nbc january year</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>past month hollywood effectively boycotted glo...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2022-09-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/biden-us-forces...</td>\n",
       "      <td>Biden Says U.S. Forces Would Defend Taiwan If ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>President issues vow as tensions with China rise.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-09-19 00:00:00</td>\n",
       "      <td>biden u force defend taiwan china</td>\n",
       "      <td>biden say force would defend taiwan china invaded</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>president issue vow tension china rise</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2022-09-19 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "1  https://www.huffpost.com/entry/funniest-parent...   \n",
       "2  https://www.huffpost.com/entry/dodgers-basebal...   \n",
       "3  https://www.huffpost.com/entry/golden-globes-r...   \n",
       "4  https://www.huffpost.com/entry/biden-us-forces...   \n",
       "\n",
       "                                            headline       category  \\\n",
       "0  23 Of The Funniest Tweets About Cats And Dogs ...         COMEDY   \n",
       "1  The Funniest Tweets From Parents This Week (Se...      PARENTING   \n",
       "2  Maury Wills, Base-Stealing Shortstop For Dodge...         SPORTS   \n",
       "3  Golden Globes Returning To NBC In January Afte...  ENTERTAINMENT   \n",
       "4  Biden Says U.S. Forces Would Defend Taiwan If ...       POLITICS   \n",
       "\n",
       "                                   short_description           authors  \\\n",
       "0  \"Until you have a dog you don't understand wha...     Elyse Wanshel   \n",
       "1  \"Accidentally put grown-up toothpaste on my to...  Caroline Bologna   \n",
       "2  Maury Wills, who helped the Los Angeles Dodger...   Beth Harris, AP   \n",
       "3  For the past 18 months, Hollywood has effectiv...               NaN   \n",
       "4  President issues vow as tensions with China rise.               NaN   \n",
       "\n",
       "                  date                               link  \\\n",
       "0  2022-09-23 00:00:00   funniest tweet cat dog september   \n",
       "1  2022-09-23 00:00:00           funniest parenting tweet   \n",
       "2  2022-09-20 00:00:00          dodger baseball obit will   \n",
       "3  2022-09-20 00:00:00            golden globe return nbc   \n",
       "4  2022-09-19 00:00:00  biden u force defend taiwan china   \n",
       "\n",
       "                                            headline       category  \\\n",
       "0                        funniest tweet cat dog week         COMEDY   \n",
       "1                         funniest tweet parent week      PARENTING   \n",
       "2                     maury will shortstop dodger dy         SPORTS   \n",
       "3            golden globe returning nbc january year  ENTERTAINMENT   \n",
       "4  biden say force would defend taiwan china invaded       POLITICS   \n",
       "\n",
       "                                   short_description           authors  \\\n",
       "0                         dog understand could eaten     elyse wanshel   \n",
       "1  accidentally put toothpaste toddler toothbrush...  caroline bologna   \n",
       "2  maury helped los angeles dodger win three worl...    beth harris ap   \n",
       "3  past month hollywood effectively boycotted glo...              <NA>   \n",
       "4             president issue vow tension china rise              <NA>   \n",
       "\n",
       "                  date  \n",
       "0  2022-09-23 00:00:00  \n",
       "1  2022-09-23 00:00:00  \n",
       "2  2022-09-20 00:00:00  \n",
       "3  2022-09-20 00:00:00  \n",
       "4  2022-09-19 00:00:00  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = pd.read_csv('processed_data.csv')\n",
    "#compare the first 5 rows of the original and preprocessed data, each column side by side\n",
    "pd.concat([original, df], axis=1).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5cd2c",
   "metadata": {},
   "source": [
    "# Text Vectorization: Convert text data to numerical features (TF-IDF or word embeddings)\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency, a numerical statistic used in natural language processing (NLP) to evaluate the importance of a word in a document within a corpus.\n",
    "\n",
    "Here's a breakdown of how TF-IDF works:\n",
    "\n",
    "1. **Term Frequency (TF):** It measures how often a term (word) appears in a document. It's calculated by dividing the number of times a term appears in a document by the total number of terms in that document. The idea is that the more frequent a term is in a document, the more important it might be.\n",
    "$$\n",
    "  \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} \n",
    " $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):** This part of the formula measures the significance of a term across a collection of documents (corpus). It penalizes the words that appear too frequently across documents and gives more weight to terms that are rare in the corpus. It's calculated as the logarithm of the ratio between the total number of documents and the number of documents containing the term, then adding 1 to avoid division by zero.\n",
    "\n",
    "   $$  \\text{IDF}(t, D) = \\log{\\left(\\frac{\\text{Total number of documents in corpus } D}{\\text{Number of documents containing term } t}\\right)} + 1 $$\n",
    "\n",
    "3. **TF-IDF:** This is the product of TF and IDF. It gives a high weight to terms that are frequent in a specific document but relatively rare in the entire corpus. Terms that occur frequently across all documents get lower weights.\n",
    "\n",
    "   $$  \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)  $$\n",
    "\n",
    "Using TF-IDF, you can represent each document as a numerical vector where each dimension represents a term and its importance in that document. This technique is widely used in information retrieval, text mining, and search engine optimization, helping to determine the relevance of a document to a query or to analyze the significance of terms within documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef2626af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bada35aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6935696202531646\n",
      "Random Forest Accuracy: 0.6474936708860759\n"
     ]
    }
   ],
   "source": [
    "#concatenate the link, headline, short_description, authors columns into a new column called text\n",
    "df['text'] = df['link'] + ' ' + df['headline'] + ' ' + df['short_description'] + ' ' + df['authors']\n",
    "df['text'] = df['text'].fillna('')\n",
    "X = df['text']\n",
    "y = df['category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Model building: Choose and train a classifier\n",
    "vectorizer = TfidfVectorizer()  # Use TF-IDF vectorizer for text to numerical feature conversion\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train_vec, y_train)\n",
    "logistic_predictions = logistic_model.predict(X_test_vec)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "print(\"Logistic Regression Accuracy:\", logistic_accuracy)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,  # Reduced number of trees\n",
    "    min_samples_leaf=2,  # More samples at each leaf\n",
    "    bootstrap=True,   # Enable bootstrapping\n",
    "    n_jobs=-1         # Use all available cores\n",
    ")\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_vec)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bfa9070",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m      9\u001b[0m rf_grid \u001b[38;5;241m=\u001b[39m GridSearchCV(rf, rf_params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mrf_grid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Best parameters and score\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_grid\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example: Hyperparameter tuning for RandomForest\n",
    "rf_params = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "rf_grid.fit(X_train_vec, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", rf_grid.best_params_)\n",
    "print(\"Best Score:\", rf_grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = rf_grid.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
